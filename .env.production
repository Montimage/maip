# ==============================================================================
# NDR Application - Production Environment Configuration
# ==============================================================================
# This file is configured for AWS EC2 Docker deployment
# Copy this file to .env and configure with your production values
#
# DEPLOYMENT CONTEXT: AWS EC2 with Docker Compose
# - Docker networking with service names (redis, ndr_server, etc.)
# - Caddy reverse proxy for HTTPS
# - Public domain: https://ndr.montimage.eu
# ==============================================================================

# ------------------------------------------------------------------------------
# SERVER CONFIGURATION
# ------------------------------------------------------------------------------
# Server protocol and networking
PROTOCOL=HTTP
SERVER_HOST=0.0.0.0
SERVER_PORT=31057
MODE=SERVER

# Docker environment flags
# DOCKER_ENV=true: Running in Docker container
# USE_SUDO=false: No sudo needed in Docker
DOCKER_ENV=true
USE_SUDO=false

# Python command for ML/analysis tasks
PYTHON_CMD=python3

# ------------------------------------------------------------------------------
# FILE UPLOAD SECURITY CONFIGURATION
# ------------------------------------------------------------------------------
# Maximum PCAP file size in bytes (10485760 = 10MB)
MAX_PCAP_SIZE=10485760

# ------------------------------------------------------------------------------
# QUEUE & REDIS CONFIGURATION
# ------------------------------------------------------------------------------
# Redis connection URL
# IMPORTANT: Use Docker service name 'redis' (not container name or localhost)
# Docker: redis://redis:6379
# Local: redis://127.0.0.1:6379
REDIS_URL=redis://redis:6379

# Enable queue-based processing (recommended for production)
USE_QUEUE_BY_DEFAULT=true

# ------------------------------------------------------------------------------
# AI ASSISTANT CONFIGURATION (Optional)
# ------------------------------------------------------------------------------
# OpenAI API key for AI assistant features (XAI explanations, flow analysis)
# Leave empty to use Ollama (local LLM) instead
OPENAI_API_KEY=

# OpenAI model to use (gpt-4o-mini, gpt-4o, gpt-3.5-turbo)
OPENAI_MODEL=gpt-4o-mini

# Token limit per user (30000 recommended for production)
# Admins have unlimited tokens
USER_TOKEN_LIMIT=30000

# Ollama Configuration (Free Local LLM Alternative)
# If OPENAI_API_KEY is not set, the system automatically uses Ollama
# Docker: Use host.docker.internal or host's IP address
# Local: Use localhost:11434
# Uncomment the appropriate URL for your setup:
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1

# ------------------------------------------------------------------------------
# CLIENT CONFIGURATION (REACT_APP_* variables)
# ------------------------------------------------------------------------------
# API URL that the React client uses to connect to the backend
# IMPORTANT: This is the public-facing URL users access in their browser
#
# Production with HTTPS (Caddy reverse proxy): https://your-domain.com
# Production with HTTP: http://YOUR_EC2_PUBLIC_IP:31057
# Local development: http://localhost:31057
#
# Current production deployment:
REACT_APP_API_URL=https://ndr.montimage.eu
#
# Alternative configurations (commented out):
#REACT_APP_API_URL=http://localhost:31057

# ------------------------------------------------------------------------------
# CORS CONFIGURATION
# ------------------------------------------------------------------------------
# Additional allowed origins for CORS (comma-separated)
# The server automatically allows:
# - localhost:3000 (development)
# - Origins derived from REACT_APP_API_URL
# Add any additional origins here if needed
CORS_ALLOWED_ORIGINS=

# ------------------------------------------------------------------------------
# AUTHENTICATION (Optional - Clerk)
# ------------------------------------------------------------------------------
# Clerk authentication service configuration
# Leave REACT_APP_CLERK_PUBLISHABLE_KEY empty to disable authentication
REACT_APP_CLERK_PUBLISHABLE_KEY=

# Admin user configuration (comma-separated lists)
REACT_APP_ADMIN_EMAILS=

# Admin user IDs from Clerk (comma-separated)
REACT_APP_ADMIN_USER_IDS=

# Admin organization ID (optional)
REACT_APP_ADMIN_ORG_ID=

# ------------------------------------------------------------------------------
# NATS CONFIGURATION (Optional - Advanced Messaging)
# ------------------------------------------------------------------------------
# NATS server connection for real-time alert distribution
# Leave empty if not using NATS
NATS_URL=

# NATS credentials for ingestor (publishes alerts)
NATS_USER_INGESTOR=
NATS_PASS_INGESTOR=

# NATS credentials for viewer (subscribes to alerts)
NATS_USER_VIEWER=
NATS_PASS_VIEWER=

# NATS subject/topic for alert messages
NATS_SUBJECT=

# ------------------------------------------------------------------------------
# WORKER CONCURRENCY CONFIGURATION (Performance Tuning)
# ------------------------------------------------------------------------------
# Worker counts for Bull queue processing
# Adjust based on your EC2 instance CPU cores

# For 4-core system (10-20 concurrent users)
FEATURE_WORKERS=3
TRAINING_WORKERS=2
PREDICTION_WORKERS=2
RULEBASED_WORKERS=2
XAI_WORKERS=1
ATTACK_WORKERS=1
RETRAIN_WORKERS=1

# For 8-core system (20-40 concurrent users)
#FEATURE_WORKERS=6
#TRAINING_WORKERS=3
#PREDICTION_WORKERS=4
#RULEBASED_WORKERS=2
#XAI_WORKERS=2
#ATTACK_WORKERS=2
#RETRAIN_WORKERS=2

# ------------------------------------------------------------------------------
# LOGGING & PERFORMANCE CONFIGURATION
# ------------------------------------------------------------------------------
# Node.js environment (production = optimized, development = verbose)
NODE_ENV=production

# Verbose logging (false for production performance, true for debugging)
VERBOSE_LOGS=false

# Node.js memory limit (increase for large workloads)
# 4096 = 4GB, 8192 = 8GB
NODE_OPTIONS=--max-old-space-size=4096